{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e0fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded t5-small\n",
      "üìä Vocabulary size: 32,100\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"‚úÖ Loaded {model_name}\")\n",
    "print(f\"üìä Vocabulary size: {tokenizer.vocab_size:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51a7d",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486a2993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/fine-tuning-oI3l7_A7-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from get_dataset import create_comprehensive_yoda_dataset\n",
    "\n",
    "yoda_dataset = create_comprehensive_yoda_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55f6cf",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7743242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing yoda dataset:   0%|          | 0/50 [00:00<?, ? examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/fine-tuning-oI3l7_A7-py3.13/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Tokenizing yoda dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 2048.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "add prefix to the dataset\n",
    "'''\n",
    "def preprocess_yoda_function(examples):\n",
    "    \"\"\"\n",
    "    Format: \"translate to yoda: [normal text]\" ‚Üí \"[yoda text]\"\n",
    "    \"\"\"\n",
    "    # Create task-specific inputs with clear task description\n",
    "    inputs = [f\"translate to yoda voice: {text}\" for text in examples[\"input\"]]\n",
    "    targets = examples[\"target\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "    \n",
    "    # Add labels for training\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# process the dataset\n",
    "tokenized_yoda_dataset = yoda_dataset.map(\n",
    "    preprocess_yoda_function,\n",
    "    batched=True,\n",
    "    remove_columns=yoda_dataset.column_names,\n",
    "    desc=\"Tokenizing yoda dataset\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddd6d5",
   "metadata": {},
   "source": [
    "### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cb9534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train dataset: 40 examples\n",
      "üìä Eval dataset:  10 examples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data (80% train, 20% eval)\n",
    "dataset_size = len(tokenized_yoda_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "train_dataset = tokenized_yoda_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_yoda_dataset.select(range(train_size, dataset_size))\n",
    "\n",
    "print(f\"üìä Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"üìä Eval dataset:  {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daa045",
   "metadata": {},
   "source": [
    "## Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936d58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust metrics function created\n"
     ]
    }
   ],
   "source": [
    "## metric evaluation function\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics_robust(eval_pred):\n",
    "    \"\"\"\n",
    "    Robust metrics computation that handles early training issues\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Filter out empty predictions (common early in training)\n",
    "    valid_pairs = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        if pred.strip() and label.strip():  # Both must be non-empty\n",
    "            valid_pairs.append((pred.strip(), label.strip()))\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        # No valid predictions yet - return zero scores\n",
    "        return {\n",
    "            \"bleu\": 0.0,\n",
    "            \"valid_predictions\": 0,\n",
    "            \"total_predictions\": len(decoded_preds)\n",
    "        }\n",
    "    \n",
    "    # Separate valid predictions and references\n",
    "    valid_preds, valid_refs = zip(*valid_pairs)\n",
    "    \n",
    "    try:\n",
    "        # Try to compute BLEU\n",
    "        bleu_metric = evaluate.load(\"bleu\")\n",
    "        result = bleu_metric.compute(\n",
    "            predictions=list(valid_preds), \n",
    "            references=[[ref] for ref in valid_refs]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"bleu\": result[\"bleu\"] if result[\"bleu\"] is not None else 0.0,\n",
    "            \"valid_predictions\": len(valid_pairs),\n",
    "            \"total_predictions\": len(decoded_preds)\n",
    "        }\n",
    "        \n",
    "    except (ZeroDivisionError, ValueError) as e:\n",
    "        # Fallback: simple accuracy-like metric\n",
    "        print(f\"BLEU computation failed: {e}. Using fallback metric.\")\n",
    "        \n",
    "        # Simple token-level accuracy\n",
    "        correct_tokens = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for pred, ref in valid_pairs:\n",
    "            pred_tokens = pred.split()\n",
    "            ref_tokens = ref.split()\n",
    "            \n",
    "            for i in range(min(len(pred_tokens), len(ref_tokens))):\n",
    "                total_tokens += 1\n",
    "                if pred_tokens[i] == ref_tokens[i]:\n",
    "                    correct_tokens += 1\n",
    "        \n",
    "        accuracy = correct_tokens / max(total_tokens, 1)\n",
    "        \n",
    "        return {\n",
    "            \"bleu\": accuracy,  # Use accuracy as proxy for BLEU\n",
    "            \"valid_predictions\": len(valid_pairs),\n",
    "            \"total_predictions\": len(decoded_preds)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Robust metrics function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e611af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed training arguments created\n"
     ]
    }
   ],
   "source": [
    "## training config\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# More conservative training arguments\n",
    "training_args_fixed = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./yoda-translator\",\n",
    "    eval_strategy=\"steps\",  # Change to steps instead of epoch\n",
    "    eval_steps=50,          # Evaluate every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=10,     # Reduced epochs\n",
    "    per_device_train_batch_size=2,  # Smaller batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,      # Lower learning rate\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,         # Fewer warmup steps\n",
    "    \n",
    "    # Generation settings\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,  # Shorter max length\n",
    "    generation_num_beams=2,    # Fewer beams for stability\n",
    "    \n",
    "    # Performance\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=None,\n",
    "    \n",
    "    # Skip first evaluations to avoid early errors\n",
    "    skip_memory_metrics=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fixed training arguments created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73dffc",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7954673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Yoda Trainer created and ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/k40b_cq90vj9wb36fl6cz3_w0000gn/T/ipykernel_9506/2679246393.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args_fixed,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_robust,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Yoda Trainer created and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c8d5",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d49c8a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Yoda translator...\n",
      "This may take 15-30 minutes depending on your hardware...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Valid Predictions</th>\n",
       "      <th>Total Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.107000</td>\n",
       "      <td>1.396541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.168800</td>\n",
       "      <td>0.985728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.862383</td>\n",
       "      <td>0.129307</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>0.819774</td>\n",
       "      <td>0.169352</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Yoda translator saved to: ./yoda-translator-final\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Training Yoda translator...\")\n",
    "print(\"This may take 15-30 minutes depending on your hardware...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = \"./yoda-translator-final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Yoda translator saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b67638",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9672174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing model from: ./yoda-translator-final\n",
      "‚úÖ Model loaded successfully\n",
      "\n",
      "üê∏ YODA TRANSLATION TESTS:\n",
      "----------------------------------------\n",
      "Normal: You are very strong.\n",
      "Yoda:   Strong you are.\n",
      "\n",
      "Normal: I will help you.\n",
      "Yoda:   Help you, I will.\n",
      "\n",
      "Normal: The Force is powerful.\n",
      "Yoda:   Powerful the Force is.\n",
      "\n",
      "Normal: Trust your feelings.\n",
      "Yoda:   Trust your feelings, you must.\n",
      "\n",
      "Normal: We must be patient.\n",
      "Yoda:   Patient, we must be.\n",
      "\n",
      "\n",
      "üéâ Training complete! May the Force be with your model! ‚ú®\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_yoda_model(model_path):\n",
    "    \"\"\"Test the trained model\"\"\"\n",
    "    \n",
    "    print(f\"\\nüß™ Testing model from: {model_path}\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    try:\n",
    "        trained_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        trained_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    def translate_to_yoda_fixed(text):\n",
    "        \"\"\"Translate using the trained model\"\"\"\n",
    "        input_text = f\"translate to yoda: {text}\"\n",
    "        \n",
    "        inputs = trained_tokenizer(\n",
    "            input_text, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=128, \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = trained_model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=64,  # Shorter for stability\n",
    "                num_beams=2,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        result = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return result if result else \"Strong with errors, this model is. Try again, you must.\"\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"You are very strong.\",\n",
    "        \"I will help you.\",\n",
    "        \"The Force is powerful.\",\n",
    "        \"Trust your feelings.\",\n",
    "        \"We must be patient.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüê∏ YODA TRANSLATION TESTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        try:\n",
    "            yoda_result = translate_to_yoda_fixed(sentence)\n",
    "            print(f\"Normal: {sentence}\")\n",
    "            print(f\"Yoda:   {yoda_result}\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating '{sentence}': {e}\")\n",
    "    \n",
    "    return translate_to_yoda_fixed\n",
    "\n",
    "# Test the model\n",
    "translate_function = test_yoda_model('./yoda-translator-final')\n",
    "\n",
    "print(\"\\nüéâ Training complete! May the Force be with your model! ‚ú®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a35a5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "Input: I will train you well.\n",
      "Yoda Translation: Train you well, I will.\n"
     ]
    }
   ],
   "source": [
    "## standalone function\n",
    "def translate_to_yoda(text, model_path='./yoda-translator-final'):\n",
    "    try:\n",
    "        trained_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        trained_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        raise e\n",
    "    \n",
    "\n",
    "    input_text = f\"translate to yoda voice: {text}\"\n",
    "        \n",
    "    inputs = trained_tokenizer(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=128, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=64,  # Shorter for stability\n",
    "            num_beams=2,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    result = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result if result else \"Strong with errors, this model is. Try again, you must.\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_text = \"I will train you well.\"\n",
    "yoda_translation = translate_to_yoda(input_text)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Yoda Translation: {yoda_translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ad6e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Nottingham/fine_tuning/src/t5_seq_2_seq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d629840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-oI3l7_A7-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
